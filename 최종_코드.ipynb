{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "최종 코드.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAJ3qc9tx1qc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/gdrive')==False:\n",
        "  drive.mount('/content/gdrive')\n",
        "  print('Google Drive is mounted\\n')\n",
        "else:\n",
        "  print('Google Drive is already mounted\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mTZwiFVx37m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "# 랜덤시드 고정시키기\n",
        "np.random.seed(777)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=15,\n",
        "                                  width_shift_range=0.1,\n",
        "                                  height_shift_range=0.1,\n",
        "                                  shear_range=0.5,\n",
        "                                  zoom_range=[0.8, 2.0],\n",
        "                                  horizontal_flip=True,\n",
        "                                  vertical_flip=True,                  # data augmentation 출처 https://tykimos.github.io/2017/06/10/CNN_Data_Augmentation/\n",
        "                                  fill_mode='nearest'\n",
        "                                  ) \n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/Colab Notebooks/house/handwriting_shape/train',\n",
        "        target_size=(24, 24),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical') # trian data\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)  # nomalization\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/Colab Notebooks/house/handwriting_shape/test',\n",
        "        target_size=(24, 24),    \n",
        "        batch_size=32,\n",
        "        class_mode='categorical') #test data\n",
        "   \n",
        "from keras.models import Sequential, Model\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Input, BatchNormalization, AveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "classes = 4\n",
        "filter_size = 16  # 필터 개수\n",
        "kernel_size = (3,3) # 필터 사이즈\n",
        "pool_size = (2,2)\n",
        "input_shape = (24,24,3)\n",
        "def getModel(input_shape):\n",
        "  \n",
        "  def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
        "    filters1, filters2, filters3 = filters\n",
        "    \n",
        "    conv_name_base = 'res' + str(stage) + block +  '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block +  '_branch'\n",
        "    \n",
        "    x = Conv2D(filters1, (1,1), name = conv_name_base +'2a')(input_tensor)\n",
        "    x = BatchNormalization(name = bn_name_base+'2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters2, kernel_size,padding='same',name = conv_name_base+'2b')(x)\n",
        "    x = BatchNormalization(name = bn_name_base +'2b')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    \n",
        "    x = Conv2D(filters3,(1,1),name = conv_name_base+'2c')(x)\n",
        "    x = BatchNormalization(name = bn_name_base +'2c')(x)\n",
        "    \n",
        "    x = layers.add([x,input_tensor])\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  def conv_block(input_tensor,kernel_size, filters,stage, block,strides=(2,2)):\n",
        "    filters1, filters2 , filters3 = filters\n",
        "    conv_name_base = 'res' + str(stage)+block+'_branch'\n",
        "    bn_name_base = 'bn' +str(stage) +block+'_branch'\n",
        "    \n",
        "    x = Conv2D(filters1, (1,1),strides = strides, name = conv_name_base +'2a')(input_tensor)\n",
        "    x = BatchNormalization(name = bn_name_base +'2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    x = Conv2D(filters2, kernel_size,padding = 'same', name = conv_name_base +'2b')(x)\n",
        "    x = BatchNormalization(name = bn_name_base+'2b')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    x = Conv2D(filters3,(1,1), name = conv_name_base +'2c')(x)\n",
        "    x = BatchNormalization(name = bn_name_base+'2c')(x)\n",
        "    \n",
        "    shortcut = Conv2D(filters3, (1,1), strides = strides, name = conv_name_base+'1')(input_tensor)\n",
        "    shortcut = BatchNormalization(name = bn_name_base+'1')(shortcut)\n",
        "    \n",
        "    x = layers.add([x,shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  i = Input(shape = input_shape)\n",
        "  x = Conv2D(64, (7,7),strides = (2,2), padding = 'same', name = 'conv1')(i)\n",
        "  x = BatchNormalization(name = 'bn_conv1')(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D((3,3), strides =(2,2))(x)\n",
        " \n",
        "  x = conv_block(x,3,[64,64,256],stage = 2 , block = 'a', strides =(1,1))\n",
        "  x = identity_block(x,3,[64,64,256], stage = 2, block = 'b')\n",
        "  x = identity_block(x,3,[64,64,256], stage = 2, block = 'c')\n",
        "  x = conv_block(x,3,[128,128,512], stage = 3, block = 'a')\n",
        "  x = identity_block(x,3,[128,128,512], stage = 3, block = 'b')\n",
        "  x = identity_block(x,3,[128,128,512], stage = 3, block = 'c')\n",
        "  x = identity_block(x,3,[128,128,512], stage = 3, block = 'd')\n",
        "  \n",
        "  x = AveragePooling2D((2,2), name = 'avg_pool')(x)\n",
        "  \n",
        "  x = Flatten()(x)\n",
        "  x = Dense(64,input_dim= 64, kernel_regularizer = regularizers.l2(0.01))(x)\n",
        "  x = Dense(classes,activation = \"softmax\", name = \"softmax\")(x)\n",
        "  \n",
        "  \n",
        "  return Model(i,x, name = 'Resnet')\n",
        "        \n",
        "\n",
        "m = getModel(input_shape)\n",
        "\n",
        "\n",
        "lr = 0.00123  # best learning rate from random search\n",
        "\n",
        "# 모델 엮기\n",
        "m.compile(loss='categorical_crossentropy', optimizer= Adam(lr), metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습시키기\n",
        "history = m.fit_generator(\n",
        "         train_generator,\n",
        "        steps_per_epoch = 100,\n",
        "        epochs=200, validation_data = test_generator, validation_steps = 4\n",
        "       )\n",
        "\n",
        "# 모델 평가하기\n",
        "print(\"-- Evaluate --\")\n",
        "\n",
        "scores = m.evaluate_generator(\n",
        "            test_generator, \n",
        "            steps = 4)\n",
        "\n",
        "print(\"%s: %.2f%%\" %(m.metrics_names[1], scores[1]*100))\n",
        "  # 훈련의 정확도, 손실 그래프 그리기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss =  history.history['loss']\n",
        "val_loss =  history.history['val_loss']\n",
        "epochs = range(1,len(acc)+1)\n",
        "plt.plot(epochs,acc,'bo',label = 'Training acc')\n",
        "plt.plot(epochs,val_acc,'b',label = 'Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo',label = 'Training loss')\n",
        "plt.plot(epochs,val_loss,'b',label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "m.save('m.h5') # 학습 모델 저장"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoS9AU6tx654",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "m = load_model('m.h5')  #학습 모델 불러오기\n",
        "Test_datagen = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "Test_generator = Test_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/Colab Notebooks/house/handwriting_shape/Test',\n",
        "        target_size=(24, 24),    \n",
        "        batch_size=32,\n",
        "        class_mode='categorical') # 실제로 찍은 테스트 데이터 불러오기\n",
        "scores = m.evaluate_generator(\n",
        "            Test_generator, \n",
        "            steps = 4)\n",
        "print(\"%s: %.2f%%\" %(m.metrics_names[1], scores[1]*100)) # 실제 데이터 accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D8b8GIKyFeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_width, img_height  =24, 24\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      )\n",
        "\n",
        "# Create a generator for prediction\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "      '/content/gdrive/My Drive/Colab Notebooks/house/handwriting_shape/Test',#경로 validation으로 수정하기\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=10,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        " \n",
        "# Get the filenames from the generator\n",
        "fnames = validation_generator.filenames\n",
        " \n",
        "# Get the ground truth from generator\n",
        "ground_truth = validation_generator.classes\n",
        " \n",
        "# Get the label to class mapping from the generator\n",
        "label2index = validation_generator.class_indices\n",
        " \n",
        "# Getting the mapping from class index to class label\n",
        "idx2label = dict((v,k) for k,v in label2index.items())\n",
        " \n",
        "# Get the predictions from the model using the generator\n",
        "predictions = m.predict_generator(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)\n",
        "predicted_classes = np.argmax(predictions,axis=1)\n",
        " \n",
        "errors = np.where(predicted_classes != ground_truth)[0]\n",
        "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))\n",
        "right = np.where(predicted_classes == ground_truth)[0] \n",
        "# Show the errors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "for i in range(len(errors)):\n",
        "    pred_class = np.argmax(predictions[errors[i]])\n",
        "    pred_label = idx2label[pred_class]\n",
        "     \n",
        "    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
        "        fnames[errors[i]].split('/')[0],\n",
        "        pred_label,\n",
        "        predictions[errors[i]][pred_class])\n",
        "     \n",
        "    original = image.load_img('{}/{}'.format('/content/gdrive/My Drive/Colab Notebooks/house/handwriting_shape/Test',fnames[errors[i]]))\n",
        "    plt.figure(figsize=[7,7])\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.imshow(original)\n",
        "    plt.show()\n",
        "\n",
        "        \n",
        "for i in range(len(right)):\n",
        "    pred_class_right = np.argmax(predictions[right[i]])\n",
        "    pred_label_right = idx2label[pred_class_right]\n",
        "     \n",
        "    title_right = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
        "        fnames[right[i]].split('/')[0],\n",
        "        pred_label_right,\n",
        "        predictions[right[i]][pred_class_right])\n",
        "     \n",
        "    original_right = image.load_img('{}/{}'.format('/content/gdrive/My Drive/Colab Notebooks/house/handwriting_shape/Test',fnames[right[i]]))\n",
        "    plt.figure(figsize=[7,7])\n",
        "    plt.axis('off')\n",
        "    plt.title(title_right)\n",
        "    plt.imshow(original_right)\n",
        "    plt.show()\n",
        "    \n",
        "cm = confusion_matrix(ground_truth, predicted_classes)\n",
        "cm\n",
        "TruePositive = np.diag(cm)\n",
        "print(\"TP is\" + str(TruePositive) )\n",
        "\n",
        "FalsePositive = []\n",
        "for i in range(4):\n",
        "    FalsePositive.append(sum(cm[:,i]) - cm[i,i])\n",
        "\n",
        "print(\"FP is\" + str(FalsePositive) )\n",
        "        \n",
        "FalseNegative = []\n",
        "for i in range(4):\n",
        "    FalseNegative.append(sum(cm[i,:]) - cm[i,i])\n",
        "    \n",
        "print(\"FN is\" + str(FalseNegative) )\n",
        "\n",
        "TrueNegative = []\n",
        "for i in range(4):\n",
        "    temp = np.delete(cm, i, 0)   # delete ith row\n",
        "    temp = np.delete(temp, i, 1)  # delete ith column\n",
        "    TrueNegative.append(sum(sum(temp)))\n",
        "   \n",
        "print(\"TN is\" + str(TrueNegative) )   \n",
        "\n",
        "target_names = ['Glass', 'Metal','Paper','Plastic']\n",
        "print(classification_report(ground_truth, predicted_classes, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}